{{> header}}
<a href="/">Home</a>
<header>
    <h1>Slack Message Clustering For Kubernetes Support Community</h1>
    <p>Created:
        <strong>June 30, 2020</strong></p>
    <p>Last Modified: <strong>July 12, 2020</strong></p>
</header>
<div>An analysis of messages sent to '#velero' channel in kubernetes slack
    with the goal of identifying any clustering and extracting FAQs.
</div>

<section>
    <h2>Business Impact</h2>
    <p>The business stakeholder in this analysis is an open source engineering team called Velero. The engineers spend
        a unscientific "good amount of time" every week answering questions from the community in slack. The initial
        goal for this analysis is to see if there are any Frequently Asked Questions (FAQ's) that can be discerned
        from the message data. The resultant FAQ's could be referenced by the community members who are answering
        questions
        as a quick way to share information without repeating the same answer many times. It is hypothesized this could
        result in a time savings for support engineers.</p>
</section>

<section>
    <h2>Community Impact</h2>
    <p>Creating a FAQ document would help improve consistency in support messaging, ensuring that similar questions
        are answered with similar guidance. Additionally, if engineers working on the team spend less time answering
        common questions, they could spend more time and give more consideration to investigating unfamiliar problems
        posed by users.
    </p>
</section>
<section>
    <h2>Data Preparation</h2>
    <p>
        This dataset is a collection of messages in the public #velero channel taken from the Kubernetes Slack
        for a nine month period between July 31, 2019 and April 20, 2020. There is one folder
        corresponding to each channel and within each folder are several files, each one containing all of the messages
        for that day. The data is in json format. I will not include technicalities of parsing the data here, only the
        more interesting decisions relating to data cleaning.
    </p>
</section>
<section>
    <h3>Basic Language cleanup</h3>
    <h4>Removing Messages from team members</h4>
    <p>As stated in the business impact section, the goal of this analysis is to find FAQ's from the product users. For
        this reason, it is ok to remove messages from the engineering support team itself. In later steps, we are
        removing
        messages that are only questions as well. So any questions from the engineering support team are more likely to
        be
        clarifying questions for the user, for instance asking what version of the product they are running or asking if
        they could run a command and post the output. This is not the types of questions we are looking for and will
        at best create confusion in the data and at best create false clusters. </p>
    <h4>URL Removal, User Tag Removal, Punctuation Removal</h4>
    <p>
        All occurrences of "http" or "https" were removed. The rest of the url string was left in the text because users
        sending messages and referencing the same link (possibly a user document) might be talking about the same thing
        so urls could provide a useful signal.<br>
        All user tags were removed. In slack you can "mention" another user in your message. Though user tags may convey
        some signal, they were removed to start. <br>
        All punctuation was removed. <br>
    </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
    <code class="language-py code-snippet" contenteditable spellcheck="false">
    import re
    import string
    self.text = self.text.replace("https://", "")
    self.text = self.text.replace("http://", "")
    users_regex = re.compile(r"\<\@[A-Za-z\d]+\>")
    self.text = re.sub(users_regex, "", self.text)
    self.text = self.text.translate(str.maketrans('', '', string.punctuation))
    </code>
  </pre>
    </figure>
    <h4>Lemmatization</h4>
    <p>
        There are tradeoffs with lemamatization vs stemming, both used to reduce the number of tokens that represent
        the same root. Ultimately in this case we are not worried about speed and we are worried about the roots having
        real semantic meaning for some of the other cleaning steps. With stemming sometimes the roots come out and are
        not real words.
    </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
    <code class="language-py code-snippet" contenteditable spellcheck="false">
    import nltk
    from nltk.stem import WordNetLemmatizer
    from nltk.corpus import wordnet

    def get_wordnet_pos(word: str):
        """Map part of speech (POS) tag to first character lemmatize() accepts"""
        tag = nltk.pos_tag([word])[0][1][0].upper()
        tag_dict = {"J": wordnet.ADJ,
                    "N": wordnet.NOUN,
                    "V": wordnet.VERB,
                    "R": wordnet.ADV}

        return tag_dict.get(tag, wordnet.NOUN)

    def lemmatize(message: str):
        word_list = nltk.word_tokenize(message)
        lemmatizer = WordNetLemmatizer()
        new_message = [lemmatizer.lemmatize(w, MessageCleaner.get_wordnet_pos(w)) for w in word_list]
        new_message = " ".join(new_message)
        return new_message
    </code>
  </pre>
    </figure>
</section>
<section>
    <h3>Vectorization</h3>
    <p>
        Before proceeding further in data preparation, we need to define some text vectorizers. Since machine learning
        algorithms deal with tabular data, we need to take our messages and transform them into rows with feature
        columns.
        This is where vectorization comes in. There are several approaches, but here two different ones were taken and
        used for different purposes. Both are pretty easy to use from scikit learn</p>
    <h4> Count Vectorizer </h4>
    <p>
        This is the most basic form of a vectorizer. Each message is converted into a series of tokens. A token may be
        one word, or it can be a group of words, called an "ngram". Ngrams are used to help capture meaning when words
        placed together can change the meaning of a word. For example "high school" appearing in a sentence has a
        different meaning than "high" and "school" appearing separately in a sentence. Here, "ngram_range" was specified
        to tell the vectorizers to form one, two, and three word ngrams.
    </p>
    <p>
        Additionally, vectorizers remove stopwords from text. These are very common words such as "and" or "but" which
        create misleading similarity between texts and do not add any meaning. Some context-specific stopwords were also
        specified for the vectorizer to remove.
    </p>
    <p>
        There are some other parameters to specify for the count vectorizer. Lowercase parameter tells the vectorizer to
        lowercase all the letters. This is almost always wanted to avoid several tokens representing the same meaning
        but differing in capitalization. Before lowercasing everything, some analysts might make an effort to identify
        proper nouns and exempt them from lowercasing but this was not done here. "min_df" parameter specifies that any
        token that appears in less than three documents should be removed. "max_df" specifies that any token appearing
        in
        more than 70% of documents should be removed. This helps to remove commonality that does not provide any
        information.
    </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
    <code class="language-py code-snippet" contenteditable spellcheck="false">
    from sklearn.feature_extraction import text
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    my_stop_words = ["velero", "backup", "hi", "hello", "hey", "use", "yeah"]
    my_stop_words = text.ENGLISH_STOP_WORDS.union(my_stop_words)
    count_vectorizer = CountVectorizer(ngram_range=(1, 3),
                                            stop_words=my_stop_words,
                                            lowercase=True,
                                            min_df=3,
                                            max_df=0.7,
                                            analyzer='word')
    </code>
  </pre>
    </figure>
    <h4> TF/IDF Vectorizer </h4>
    <p>
        Count vectorizer accomplishes our task of converting text into a tabular format but a useful transformation to
        apply to the data is Term-Frequency Inverse Document-Frequency. Without going into the details, the thinking
        behind this is tokens that are very popular in one document but are also very popular in all the other documents
        are not as important as words that appear in a document but are rare in other documents. This token is more
        likely
        to give meaning. The TF/IDF vectorization performs much lik the count vectorizer, but there is a "popularity
        penalty"
        for very popular terms. In practice performing tf/idf is almost always useful unless you do want the raw count
        data for some specific reason.
    </p>

    <figure class="code-snippet-fig horizontal-center">
        <pre>
    <code class="language-py code-snippet" contenteditable spellcheck="false">
    self.tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),
                                            stop_words=my_stop_words,
                                            lowercase=True,
                                            min_df=3,
                                            max_df=0.7,
                                            analyzer='word')
    </code>
  </pre>
    </figure>
</section>
<section>
    <h3>Message Grouping</h3>
    Once the messages are cleaned, there are about 6,000 messages. Once messages which are not questions are removed,
    there are only around 1500 messages. At the request of a business stakeholder, some analysis was done to analyze
    messages sent within a short time period from the same user that represent a "series" of messages. The reasoning is
    one of the messages in the series could be an actual question, but the subsequent messages could provide more
    details
    or context on the question that could also be useful for clustering. <br>
    The methodology is to first group the messages that form a "series". Then, if any of the messages in the series are
    a
    question, then all of the messages in the series are allowed to remain in the dataset. If no messages in the series
    are
    a question, then the entire series is removed from the dataset. After adding in this grouping logic, there were
    1800 messages remaining. So by performing this series logic, we added 300 more rows to our dataset. <br>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
    <code class="language-py code-snippet" contenteditable spellcheck="false">
    def combine_user_messages_short_time(self):
        window = datetime.timedelta(hours=4)
        series_by_username: DefaultDict[str, List[MessageSeries]] = defaultdict(list)
        series_list: List[MessageSeries] = list()
        for index, row in self.message_df.sort_values(by=['time']).iterrows():
            username = row['username']
            message_text = row['message']
            time = row['time']
            message = Message(username=username,
                              text=message_text,
                              time=time)
            if username in series_by_username:
                user_series_list = series_by_username[username]
                user_latest_series = user_series_list[-1]
                diff = time - user_latest_series.time_latest
                if diff < window:
                    user_latest_series.messages.append(message)
                    continue

            new_series = MessageSeries(username=username,
                                       messages=list())
            new_series.messages.append(message)
            series_by_username[username].append(new_series)
            series_list.append(new_series)
        self.series_by_username = series_by_username

        self.train_dialogue_act_classifier()
        # go through series_by_username and build up a dataframe
        question_acts = ['ynQuestion', 'whQuestion']
        for (username, message_serieses) in series_by_username.items():
            series = 0
            for message_series in message_serieses:
                # if any of the messages in the series are a question,
                # then add the whole group to the dataset
                message_texts = map(lambda msg: msg.text, message_series.messages)
                predictions = self.gb.predict(self.tfidf_vectorizer.transform(message_texts))
                if list(set(question_acts) & set(predictions)):
                    # there is at least one question in this series. append every message to the df.
                    for message in message_series.messages:
                        time = message.time
                        self.combined_messages.loc[len(self.combined_messages)] = [message.text,
                                                                                   message_series.username,
                                                                                   series,
                                                                                   time]
                        series += 1
    </code>
  </pre>
    </figure>
</section>
<section>
    <h3>Dialogue Act Classification</h3>
    <p>
        In the snippet above there is a line which predicts the dialogue act of each message in the dataset.
        This is done by training a gradient boosting classifier with some labled text data. There is some public
        data available from nltk which represents "chat" data and each message is tagged with a dialogue act such
        as "clarification" or "ynquestion" which represents a yes/no question. By training this model, we can then
        predict the dialogue class of each of our messages. For a start, all of the chat data was used to train the
        model.
        The model can be improved if we do train/test splits and cross-validation to tune the model. For now this
        is a first pass.
    </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
        from sklearn.ensemble import GradientBoostingClassifier
        import nltk
        def train_dialogue_act_classifier(self):
            # take 10k of the chat posts
            posts = nltk.corpus.nps_chat.xml_posts()[:10000]
            posts_text = [post.text for post in posts]
            # divide train and test in 80 20
            # train_text = posts_text[:int(len(posts_text) * 0.8)]
            # test_text = posts_text[int(len(posts_text) * 0.2):]

            # Get TFIDF features
            # we can tune this with xval and roc curves
            # for now use all the data for train. we can come back and tune this model later
            #        X_train = vectorizer.fit_transform(train_text)
            X_train = self.tfidf_vectorizer.fit_transform(posts_text)
            # X_test = vectorizer.transform(test_text)

            y_train = [post.get('class') for post in posts]
            # y = [post.get('class') for post in posts]

            # y_train = y[:int(len(posts_text) * 0.8)]
            # y_test = y[int(len(posts_text) * 0.2):]

            # Fitting Gradient Boosting classifier to the Training set
            gb = GradientBoostingClassifier(n_estimators=400, random_state=0)
            # Can be improved with Cross Validation
            gb.fit(X_train, y_train)
            self.gb = gb
            </code>
        </pre>
    </figure>
</section>
<section>
    <h2>Initial Data Exploration</h2>
    <h3>Token Counts</h3>
    <p>
        List out the most popular tokens in the dataset. Interesting variation is to change binary=True
        as a parameter to the count vectorizer. This tells the vectorizer to show only 0 or 1 showing if the
        token is present or not in each document instead of the number of appearances. This shows highest
        token count across all documents. </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
        def perform_count_vectorization(self, df_to_vectorize):
            df_count_vectorized = self.count_vectorizer.fit_transform(df_to_vectorize['message'])
            self.arr_count = df_count_vectorized.toarray()

        def perform_tfidf_vectorization(self, df_to_vectorize):

            df_tfidf_vectorized = self.tfidf_vectorizer.fit_transform(df_to_vectorize['message'])
            self.arr_tfidf = df_tfidf_vectorized.toarray()

        analyzer.perform_count_vectorization(df_to_vectorize=analyzer.combined_messages)
                feature_names = analyzer.count_vectorizer.get_feature_names()
        sum_column = analyzer.arr_count.sum(axis=0)
        count_df = pd.DataFrame({'feature_name':feature_names, 'count':sum_column})
        print(count_df.sort_values(by=['count'], ascending=False).head(20))
            </code>
        </pre>
    </figure>
    <img class="toptokens horizontal-center" src="/images/toptokens.png" alt="List of Top Tokens for Dataset">
    <p>
        Next we plot histograms to visualize the distribution of token counts
    </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
                plt.hist(np.log(sum_column))
                plt.show()

                plt.hist(sum_column)
                plt.show()
            </code>
        </pre>
    </figure>
    <img class="fiftypctwidth horizontal-center" src="/images/ngram_count_hist.png" alt="Token Count Histogram">
    <p>
        Another common way to visualize common tokens is using a wordcloud. Token size corresponds to count.
    </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
            text = " ".join(message for message in analyzer.combined_messages.message)
            from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
            stopwords = set(STOPWORDS)
            stopwords.update(["velero", "backup", "hi", "hello", "hey", "use", "yeah"])
            wordcloud = WordCloud(max_font_size=40,
                                  max_words=80,
                                  background_color="white",
                                 stopwords=stopwords).generate(text)
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis("off")
            plt.show()
            </code>
        </pre>
    </figure>
    <img class="sixtypctwidth horizontal-center" src="/images/velero_wordcloud.png" alt="Token Count Wordcloud">
</section>
<section>
    <h2>Modeling</h2>
    <p>
        While the analysis process is outlined in the post linearly, as with any project there are may loops.
        After plotting some models, we go back and adjust the cleaning or adjust the hyperparameter tuning, or
        adjust the dataset sampling. I do not detail these paths here and instead just present a summary of the
        methods which were used at some point in the process using some variation of the original dataset.
    </p>
    <h3>
        Initial kmeans clustering on tf/idf data
    </h3>
    <p>
        The very first model tried was a simple kmeans calculation on the tf/idf vectorized data. There are a few
        hyperparameters you can tune with kmeans, such as the number of iterations of centroid positioning the
        algorithm is allowed to take before it has to stop. If we were deploying a model to production we might
        want to consider some of these, but to start only k, number of clusters, was tuned. Since there are about 1800
        messages, 250 was chosen as an upper bound for number of clusters. Even with that many, its unlikely the
        clusters
        would be useful.
    </p>
    <p>
        The metric used for k tuning is sum of square distances, or inertia as scikit learn calls it. This represents
        how far away the points are from the centroid. What you want to see when you plot sum of squared distances
        across multiple values for k is an "elbow" curve. As you increase k, you should see sum of square distances
        leveling out, and getting decreasingly marginally lower as k increases (see example below). There are methods
        for finding optimal k
        that are a bit more scientific than the "elbow method" but plotting this data will help us at least get an idea
        for the range within which our optimal k, if it exists, could live.
    </p>
    <figure>
        <img class="fiftypctwidth horizontal-center" src="/images/elbowcurve.png" alt="Elbowcurve definition">
        <figcaption class="horizontal-center fiftypctwidth">Image by <a
                href="https://www.edureka.co/blog/k-means-clustering-algorithm/">Hemant Sharma</a></figcaption>
    </figure>
    <p>
        Our grid search (graphed below) did not result in a curve that shows "diminishing returns" with increasing k.
        The sum of squared distances (inertia) seems to continue to improve at a relatively constant rate as k
        increases.
        There is an interesing jump where the metric gets much worse for 90 clusters and then starts to improve again.
        This could be investigated, but again even with 90 clusters, they arent very useful. I also printed out the
        clusters
        in excel to read the messages for each cluster. They all just looked like noise. There were no useful meanings
        unique
        to clusters.
        This result is not totally unexpected. Kmeans algorithm does not do well with high dimensional data. Since
        kmeans is a distance-based algorithm,
        the more dimensions (features) in your datset, the more similar the points seem because they normalize across
        all
        the different dimensions. Our tf/idf data has around 1000 features.
    </p>
    <figure class="code-snippet-fig horizontal-center">
        <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
            def create_and_fit_kmeans_tfidf(self, num_clusters=10):
                kmeans = Kmeans(self.arr_tfidf)
                (self.kmeans_data_labels, self.kmeans_model) = kmeans.create_and_fit(num_clusters)
                return (self.kmeans_data_labels, self.kmeans_model)

            sum_of_squared_distances = []
            ks = [10,  20, 30,  40, 50, 60, 70, 80, 90, 100, 120]
            for k in ks:
                (data_labels, model) = analyzer.create_and_fit_kmeans_tfidf(k)
                sum_of_squared_distances.append(model.inertia_)

            # plot ssd
            from collections import defaultdict
            d = defaultdict()

            for idx, ssd in enumerate(sum_of_squared_distances):
                k = new_ks[idx]
                d[k] = ssd


            plt.plot(all_ks, list(d.values()), 'bx-')
            plt.xlabel('k')
            plt.ylabel('Sum_of_squared_distances')
            plt.title('Search for Optimal k')
            plt.show()
            </code>
        </pre>
    </figure>
    <img class="sixtypctwidth horizontal-center" src="/images/tfidfkmeansksearch.png" alt="kmeans grid search curve">
    <h3>Cluster Visualization</h3>
    <p>
        Even though the math tells us our clusters aren't adding any meaning, we still may want to plot them and
        visualize them to get an idea for what they look like. Since the input data is so highly dimensional, we use a
        dimensionality reduction technique to plot the data in 2 dimensions. There are a few popular tools for this,
        here we tried TSNE and PCA.
    </p>
    <h4>
        TSNE Plotting
    </h4>
    <p>
        There's probably a more clever way to specify the colors for TNSE plots but here I pass in the observations with
        full dimensionality as well as their labels from kmeans clustering. TSNE will perform the dimensionality
        reduction
        and color the points to represent their kmeans cluster.
    </p>
    <p>
        Several TSNEs were plotted using varying combinations of ngrams and k values. Only one plot is shown below
        because
        as a whole, the plots did not show anything. They showed a relatively normal distribution with no recognizable
        communities forming. For comparison sake, below the plot that was created there is an example TSNE which I
        created
        using the "goodreads" dataset of users and books they have read. Here you can see some communities forming and
        clustering
        of data points.
    </p>
    <figure class="code-snippet-fig horizontal-center">
    <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
            class TSNEPlotter:
                def __init__(self):
                    colorsarr = ['red',
                     'black',
                     'gold',
                     'darkgreen',
                     'darkorchid',
                     'thistle',
                     'teal',
                     'dodgerblue',
                     'lime',
                     'silver',
                     'hotpink',
                     'navy',
                     'greenyellow',
                     'chocolate',
                     'seagreen',
                     'fuchsia',
                     'salmon',
                     'blueviolet',
                     'orange',
                     'firebrick',
                     'yellow']

                    self.rgba_colors = list(map(colors.to_rgba, colorsarr))

                    self.legend_elements = [
                    Line2D([0], [0], marker='o', color='w', label='0', markerfacecolor=self.rgba_colors[0], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='1', markerfacecolor=self.rgba_colors[1], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='2', markerfacecolor=self.rgba_colors[2], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='3', markerfacecolor=self.rgba_colors[3], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='4', markerfacecolor=self.rgba_colors[4], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='5', markerfacecolor=self.rgba_colors[5], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='6', markerfacecolor=self.rgba_colors[6], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='7', markerfacecolor=self.rgba_colors[7], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='8', markerfacecolor=self.rgba_colors[8], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='9', markerfacecolor=self.rgba_colors[9], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='10', markerfacecolor=self.rgba_colors[10], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='11', markerfacecolor=self.rgba_colors[11], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='12', markerfacecolor=self.rgba_colors[12], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='13', markerfacecolor=self.rgba_colors[13], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='14', markerfacecolor=self.rgba_colors[14], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='15', markerfacecolor=self.rgba_colors[15], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='16', markerfacecolor=self.rgba_colors[16], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='17', markerfacecolor=self.rgba_colors[17], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='18', markerfacecolor=self.rgba_colors[18], markersize=10),
                    Line2D([0], [0], marker='o', color='w', label='19', markerfacecolor=self.rgba_colors[19], markersize=10)]


                def plot_tsne(self, input_data, labels):
                    plt.figure()
                    x_embedded = TSNE(n_components=2).fit_transform(input_data)
                    x_coords = x_embedded[:, 0]
                    y_coords = x_embedded[:, 1]
                    #optional for plotting in 3D
                    # z_coords = x_embedded[:, 2]

                    plt.scatter(x_coords, y_coords, label=labels, color=np.array(self.rgba_colors)[labels])
                    plt.legend(handles=self.legend_elements, title="kmeanscluster", fancybox=True, bbox_to_anchor=(0, .9))
                    plt.show()
            TSNEPlotter().plot_tsne(analyzer.arr_tfidf, analyzer.kmeans_data_labels)

            # optional: for plotting 3D tsne
            #%matplotlib qt
            #fig = plt.figure()
            #from mpl_toolkits.mplot3d import Axes3D

            #ax = fig.add_subplot(111, projection='3d')
            #ax.scatter(x_coords, y_coords, z_coords, color=np.array(rgba_colors)[labels])
            #plt.show()
            </code>
        </pre>
    </figure>
    <figure>
        <img class="fiftypctwidth horizontal-center" src="/images/slacktsne.png" alt="TSNE of kmeans clustering">
        <figcaption class="horizontal-center fiftypctwidth">TSNE Plot of Kmeans using k=20 and TF/IDF with ngrams 1-2
        </figcaption>
    </figure>
    <figure>
        <img class="fiftypctwidth horizontal-center" src="/images/goodreadstsne.png"
             alt="TSNE of kmeans clustering for goodreads dataset">
        <figcaption class="horizontal-center fiftypctwidth">For comparison, this is what a TSNE plot looks like when
            there are
            clusters and communities forming. This is from a dataset I analyzed from goodreads.
        </figcaption>
    </figure>
    <h4>
        PCA Plotting
    </h4>
    <p>
        PCA plot also doesn't tell us much but it uses different techniques for dimensionality reduction than TSNE
        so it is usually interesting to try it in addition to TSNE.
    </p>
    <figure class="code-snippet-fig horizontal-center">
    <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
                centers = np.array(analyzer.kmeans_model.cluster_centers_)
                labels = analyzer.kmeans_model.labels_

                def plot_pca(observations, centers):
                    random_state = 0
                    pca = PCA(n_components=2, random_state=random_state)
                    reduced_features = pca.fit_transform(observations)
                    reduced_cluster_centers = pca.transform(centers)

                    plt.figure(figsize=(8, 6))
                    plt.scatter(reduced_features[:,0], reduced_features[:,1], c=y_km)
                    plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')
                    plt.show()
            </code>
    </pre>
    </figure>
    <figure>
        <img class="fiftypctwidth horizontal-center" src="/images/pca.png"
             alt="PCA plot for slack data">
        <figcaption class="horizontal-center fiftypctwidth">PCA plot for slack data
        </figcaption>
    </figure>
    <h4>
        Manual Cluster inspection
    </h4>
    <p>
        To get a feel for the messages in each cluster, find the top few messages that are closest to the centroid of
        each cluster and export them to a csv file where we can read the messages and get a feel for how much meaning
        each cluster holds.
    </p>
    <figure class="code-snippet-fig horizontal-center">
    <pre>
            <code class="language-py code-snippet" contenteditable spellcheck="false">
                def get_distances_for_each_observation(cluster_centers, observations, labels):
                    # for each question find distance to centroid
                    pd.set_option('display.max_colwidth', None)
                    distances = []
                    for idx, observation in enumerate(observations):
                        center = centers[labels[idx]]
                        dist = euclidean(observation, center)
                        distances = distances + [dist]
                    return distances

                def write_top_qs_for_each_cluster(distances, original_df, labels, filename):
                    # add cluster labels and distance columns to each observation
                    df_with_clusters = original_df
                    df_with_clusters["cluster"] = labels
                    df_with_clusters["distance"] = distances
                    #  for each cluster, get the three observations closest to the centroid
                    grouped = df_with_clusters.groupby(by="cluster")["distance"].nlargest(3)
                    new_groups = grouped.to_frame(name = 'mean').reset_index()
                    print(new_groups.shape)
                    indeces = new_groups["level_1"].values
                    print(original_df.shape)
                    new_groups["question"] = original_df.iloc[indeces, 1].values
                    print(new_groups.head())
                    filename = filename + ".csv"
                    new_groups.to_csv(filename)

                distances = get_distances_for_each_observation(centers, analyzer.arr_tfidf, labels)
                write_top_qs_for_each_cluster(distances, analyzer.message_df, labels, "plain20kmeanstfidf1-2ngrams")

            </code></pre>
    </figure>

    <h3>Topic Modeling</h3>
    <p>
        In order to reduce the dimensionality of the data while keeping the important meaning, topic modeling was
        explored.
    </p>
</section>
{{> footer}}